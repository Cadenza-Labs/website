---
slug: "/index"
title: "Cadenza Labs"
---

Dishonesty from an AI model occurs when the model doesn’t return what it believes is true but what would satisfy another goal (e.g., what the human wants to hear). Having an unsupervised way to read a model’s beliefs from its internals would be a powerful tool for avoiding this failure mode. 

With this goal in mind, during SERI-MATS, we worked on expanding the paper Discovering Latent Knowledge in Language Models Without Supervision”. We reimplemented its codebase and created a theoretical framework expanding the method to other concepts. We consider this a very important and promising area of research, both for the near and the far future. Furthermore, having worked as a team since October 2022, we see the potential of our collaboration and how our skills complement each other. We still have a number of new ideas and many remaining from our original plan that we didn’t implement yet.

