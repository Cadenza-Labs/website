---
slug: "/"
title: "Cadenza Labs"
---

Dishonesty occurs in an AI model when its outputs differ from what it believes to be true, and instead answers in a way that satisfies another goal (e.g. what the human wants to hear). By utilizing an unsupervised way to read a modelâ€™s beliefs from its internals would be a powerful tool to avoid this failure mode. 

With this goal in mind, we expanded on the paper [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) by reimplementing its codebase and creating a theoretical framework expanding the method to other concepts. Based on our findings, we consider this area of research crucial and promising, both for the near and far future.

More about our research agenda can be found [here](/research).
