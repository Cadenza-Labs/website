---
slug: "/"
title: "Cadenza Labs"
---

Dishonesty from an AI model occurs when the model doesn’t output what it believes is true but what would satisfy another goal (e.g., what the human wants to hear). Having an unsupervised way to read a model’s beliefs from its internals would be a powerful tool for avoiding this failure mode. 

With this goal in mind, during SERI-MATS, we worked on expanding the paper [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827).
We reimplemented its codebase and created a theoretical framework expanding the method to other concepts. We consider this a very important and promising area of research, both for the near and the far future. 

More about our research agenda can be found [here](/research).
